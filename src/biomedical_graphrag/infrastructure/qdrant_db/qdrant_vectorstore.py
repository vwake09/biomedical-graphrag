from typing import Any

from openai import AsyncOpenAI
from qdrant_client import AsyncQdrantClient
from qdrant_client.models import Batch, models

from biomedical_graphrag.config import settings
from biomedical_graphrag.domain.citation import CitationNetwork
from biomedical_graphrag.domain.gene import GeneRecord
from biomedical_graphrag.domain.paper import Paper
from biomedical_graphrag.utils.logger_util import setup_logging

logger = setup_logging()


class AsyncQdrantVectorStore:
    """
    Async Qdrant client for managing collections and points.
    """

    def __init__(self) -> None:
        """
        Initialize the async Qdrant client with connection parameters.
        """
        self.url = settings.qdrant.url
        self.api_key = settings.qdrant.api_key
        self.collection_name = settings.qdrant.collection_name
        self.embedding_dimension = settings.qdrant.embedding_dimension

        self.openai_client = AsyncOpenAI(api_key=settings.openai.api_key.get_secret_value())

        self.client = AsyncQdrantClient(
            url=self.url, api_key=self.api_key.get_secret_value() if self.api_key else None
        )

    async def close(self) -> None:
        """Close the async Qdrant client."""
        await self.client.close()

    async def create_collection(self) -> None:
        """
        Create a new collection in Qdrant (async).
        Args:
                collection_name (str): Name of the collection.
                kwargs: Additional parameters for collection creation.
        """
        logger.info(f"üîß Creating Qdrant collection: {self.collection_name}")
        await self.client.create_collection(
            collection_name=self.collection_name,
            vectors_config={
                "Dense": models.VectorParams(
                    size=self.embedding_dimension, distance=models.Distance.COSINE
                )
            },
        )
        logger.info(f"‚úÖ Collection '{self.collection_name}' created successfully")

    async def delete_collection(self) -> None:
        """
        Delete a collection from Qdrant (async).
        Args:
                collection_name (str): Name of the collection.
        """
        logger.info(f"üóëÔ∏è Deleting Qdrant collection: {self.collection_name}")
        await self.client.delete_collection(collection_name=self.collection_name)
        logger.info(f"‚úÖ Collection '{self.collection_name}' deleted successfully")

    async def _dense_vectors(self, text: str) -> list[float]:
        """
        Get the embedding vector for the given text (async).
        Args:
                text (str): Input text to embed.
        Returns:
                list[float]: The embedding vector.
        """
        try:
            embedding = await self.openai_client.embeddings.create(
                model=settings.qdrant.embedding_model, input=text
            )
            return embedding.data[0].embedding
        except Exception as e:
            logger.error(f"‚ùå Failed to create embedding: {e}")
            raise

    async def upsert_points(
        self, pubmed_data: dict[str, Any], gene_data: dict[str, Any] | None = None, batch_size: int = 50
    ) -> None:
        """
        Upsert points into a collection from pubmed_dataset.json structure,
        attaching related genes from gene_dataset.json into the payload (async).
        Args:
            pubmed_data (dict): Parsed JSON data from pubmed_dataset.json.
            gene_data (dict | None): Parsed JSON data from gene_dataset.json.
            batch_size (int): Number of points to process in each batch.
        """
        papers = pubmed_data.get("papers", [])
        citation_network_dict = pubmed_data.get("citation_network", {})

        logger.info(f"üìö Starting ingestion of {len(papers)} papers with batch size {batch_size}")

        # Build PMID -> [GeneRecord] index
        pmid_to_genes: dict[str, list[GeneRecord]] = {}
        if gene_data is not None:
            genes = gene_data.get("genes", [])
            logger.info(f"üß¨ Processing {len(genes)} genes for paper-gene relationships")
            for gene in genes:
                record = GeneRecord(**gene)
                for linked_pmid in record.linked_pmids:
                    pmid_to_genes.setdefault(linked_pmid, []).append(record)
            logger.info(f"üîó Built gene index for {len(pmid_to_genes)} papers")

        total_processed = 0
        total_skipped = 0

        # Process papers in batches
        for i in range(0, len(papers), batch_size):
            batch_papers = papers[i : i + batch_size]
            batch_num = (i // batch_size) + 1
            total_batches = (len(papers) + batch_size - 1) // batch_size

            logger.info(f"üì¶ Processing batch {batch_num}/{total_batches} ({len(batch_papers)} papers)")

            batch_ids = []
            batch_payloads = []
            batch_vectors = []
            batch_skipped = 0

            for paper in batch_papers:
                pmid = paper.get("pmid")
                title = paper.get("title")
                abstract = paper.get("abstract")
                publication_date = paper.get("publication_date")
                journal = paper.get("journal")
                doi = paper.get("doi")
                authors = paper.get("authors", [])
                mesh_terms = paper.get("mesh_terms", [])

                if not title or not abstract or not pmid:
                    batch_skipped += 1
                    continue  # skip incomplete papers

                try:
                    vector = await self._dense_vectors(abstract)

                    # Get citation network for this paper if available
                    citation_info = citation_network_dict.get(pmid, {})
                    citation_network = CitationNetwork(**citation_info) if citation_info else None

                    paper_model = Paper(
                        pmid=pmid,
                        title=title,
                        abstract=abstract,
                        authors=authors,
                        mesh_terms=mesh_terms,
                        publication_date=publication_date,
                        journal=journal,
                        doi=doi,
                    )

                    payload = {
                        "paper": paper_model.model_dump(),
                        "citation_network": citation_network.model_dump() if citation_network else None,
                        "genes": [g.model_dump() for g in pmid_to_genes.get(pmid, [])],
                    }

                    batch_ids.append(int(pmid))
                    batch_payloads.append(payload)
                    batch_vectors.append(vector)

                except Exception as e:
                    logger.error(f"‚ùå Failed to process paper {pmid}: {e}")
                    batch_skipped += 1
                    continue

            # Upsert batch if we have any valid papers
            if batch_ids:
                try:
                    await self.client.upsert(
                        collection_name=self.collection_name,
                        points=Batch(
                            ids=batch_ids,
                            payloads=batch_payloads,
                            vectors={"Dense": [list(v) for v in batch_vectors]},
                        ),
                    )
                    total_processed += len(batch_ids)
                    total_skipped += batch_skipped
                    logger.info(
                        f"‚úÖ Batch {batch_num} completed: {len(batch_ids)} papers upserted, \
                            {batch_skipped} skipped"
                    )
                except Exception as e:
                    logger.error(f"‚ùå Failed to upsert batch {batch_num}: {e}")
                    raise
            else:
                logger.warning(f"‚ö†Ô∏è Batch {batch_num} had no valid papers to process")

        logger.info(
            f"üéâ Ingestion complete! Total: {total_processed} papers processed, {total_skipped} skipped"
        )
